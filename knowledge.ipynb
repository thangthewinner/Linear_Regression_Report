{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWIlVFvawLs6"
      },
      "source": [
        "**TỔNG HỢP KIẾN THỨC**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1GDm_XJ2jxm"
      },
      "source": [
        "*   Gradient descent is a first-order optimization algorithm. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient of the function at the current point. \\\n",
        "*   Linear regression is an algorithm that provides a linear relationship between an independent variable and a dependent variable to predict the outcome of future events. It is a statistical method used in data science and machine learning for predictive analysis.\n",
        "\n",
        "**Gradient Descent in Linear Regression**\\\n",
        "The main aim of gradient descent is to find the best parameters of a model which gives the highest accuracy on training as well as testing datasets.\\\n",
        "**Algorithm** \\\n",
        "t ← 0 \\\n",
        "max_iterations ← 1000 \\\n",
        "w, b ← initialize randomly \\\n",
        "while t < max_iterations do \\\n",
        "    t ← t + 1\\\n",
        "    w_t+1 ← w_t − η ∇w_t\\\n",
        "    b_t+1 ← b_t − η ∇b_t\\\n",
        "end\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_TiPKNwVz7S"
      },
      "source": [
        "*   Cost Function: \\\n",
        "We will fit the linear regression parameters theta to our dataset using gradient descent.\\\n",
        "The objective of linear regression is to minimize the cost function\\\n",
        "$$\n",
        "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( h_\\theta \\left( x^{(i)} \\right) - y^{(i)} \\right)^2\n",
        "$$\n",
        "where the hypothesis $h_\\theta$ is given by the linear \\\n",
        "$$\n",
        "h_\\theta(x) = \\theta^T x = \\theta_0 + \\theta_1 x_1\n",
        "$$\n",
        "\n",
        "*   Gradient Descent: \\\n",
        "Recall that the parameters of your model are the theta values. These are the values you will adjust to minimize cost J(theta). One way to do this is to use the batch gradient descent algorithm. In batch gradient descent, each iteration performs the update. \\\n",
        "$$\n",
        "\\theta_j = \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)} \\right) x_j^{(i)}\n",
        "$$\n",
        "simultaneously update $\\theta_j$ for all $j$ \\\n",
        "\\\n",
        "With each step of gradient descent, the parameters theta come closer to the optimal values that will achieve the lowest cost J(theta).\n",
        "*   Convergence: \\\n",
        "Consequently, $-\\nabla J(\\mathbf{w})$ points in the direction of the steepest descent. \\\n",
        "\\\n",
        "Setting $\\mathbf{s} = -\\alpha \\nabla J(\\mathbf{w})$ for a sufficiently small $\\alpha > 0$ guarantees to decrease the function:\n",
        "\n",
        "$$\n",
        "J(\\mathbf{w} + (-\\alpha \\nabla J(\\mathbf{w}))) \\approx J(\\mathbf{w}) - \\alpha \\nabla J(\\mathbf{w})^T \\nabla J(\\mathbf{w}) < J(\\mathbf{w})\n",
        "$$\n",
        "\n",
        "So the iterations of steepest descent are: \\\n",
        "$$\n",
        "\\mathbf{w}^{(i+1)} \\leftarrow \\mathbf{w}^{(i)} - \\alpha \\nabla J(\\mathbf{w}^{(i)}).\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdaAS0fiW6RW",
        "outputId": "3618fc2b-fdb5-4879-8652-8a6778db2786"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 epochs elapsed\n",
            "Current accuracy is : 0.9836456109008862\n"
          ]
        }
      ],
      "source": [
        "# Example:\n",
        "# Implementation of Gradient Descent in Linear Regression\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class Linear_Regression:\n",
        "\tdef __init__(self, X, Y):\n",
        "\t\tself.X = X\n",
        "\t\tself.Y = Y\n",
        "\t\tself.b = [0, 0]\n",
        "\n",
        "\tdef update_coeffs(self, learning_rate):\n",
        "\t\tY_pred = self.predict()\n",
        "\t\tY = self.Y\n",
        "\t\tm = len(Y)\n",
        "\t\tself.b[0] = self.b[0] - (learning_rate * ((1/m) *\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnp.sum(Y_pred - Y)))\n",
        "\n",
        "\t\tself.b[1] = self.b[1] - (learning_rate * ((1/m) *\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnp.sum((Y_pred - Y) * self.X)))\n",
        "\n",
        "\tdef predict(self, X=[]):\n",
        "\t\tY_pred = np.array([])\n",
        "\t\tif not X:\n",
        "\t\t\tX = self.X\n",
        "\t\tb = self.b\n",
        "\t\tfor x in X:\n",
        "\t\t\tY_pred = np.append(Y_pred, b[0] + (b[1] * x))\n",
        "\n",
        "\t\treturn Y_pred\n",
        "\n",
        "\tdef get_current_accuracy(self, Y_pred):\n",
        "\t\tp, e = Y_pred, self.Y\n",
        "\t\tn = len(Y_pred)\n",
        "\t\treturn 1-sum(\n",
        "\t\t\t[\n",
        "\t\t\t\tabs(p[i]-e[i])/e[i]\n",
        "\t\t\t\tfor i in range(n)\n",
        "\t\t\t\tif e[i] != 0]\n",
        "\t\t)/n\n",
        "\t# def predict(self, b, yi):\n",
        "\n",
        "\tdef compute_cost(self, Y_pred):\n",
        "\t\tm = len(self.Y)\n",
        "\t\tJ = (1 / 2*m) * (np.sum(Y_pred - self.Y)**2)\n",
        "\t\treturn J\n",
        "\n",
        "\tdef plot_best_fit(self, Y_pred, fig):\n",
        "\t\tf = plt.figure(fig)\n",
        "\t\tplt.scatter(self.X, self.Y, color='b')\n",
        "\t\tplt.plot(self.X, Y_pred, color='g')\n",
        "\t\tf.show()\n",
        "\n",
        "\n",
        "def main():\n",
        "\tX = np.array([i for i in range(11)])\n",
        "\tY = np.array([2*i for i in range(11)])\n",
        "\n",
        "\tregressor = Linear_Regression(X, Y)\n",
        "\n",
        "\titerations = 0\n",
        "\tsteps = 100\n",
        "\tlearning_rate = 0.01\n",
        "\tcosts = []\n",
        "\n",
        "\t# original best-fit line\n",
        "\tY_pred = regressor.predict()\n",
        "\tregressor.plot_best_fit(Y_pred, 'Initial Best Fit Line')\n",
        "\n",
        "\twhile 1:\n",
        "\t\tY_pred = regressor.predict()\n",
        "\t\tcost = regressor.compute_cost(Y_pred)\n",
        "\t\tcosts.append(cost)\n",
        "\t\tregressor.update_coeffs(learning_rate)\n",
        "\n",
        "\t\titerations += 1\n",
        "\t\tif iterations % steps == 0:\n",
        "\t\t\tprint(iterations, \"epochs elapsed\")\n",
        "\t\t\tprint(\"Current accuracy is :\",\n",
        "\t\t\t\tregressor.get_current_accuracy(Y_pred))\n",
        "\n",
        "\t\t\tstop = input(\"Do you want to stop (y/*)??\")\n",
        "\t\t\tif stop == \"y\":\n",
        "\t\t\t\tbreak\n",
        "\n",
        "\t# final best-fit line\n",
        "\tregressor.plot_best_fit(Y_pred, 'Final Best Fit Line')\n",
        "\n",
        "\t# plot to verify cost function decreases\n",
        "\th = plt.figure('Verification')\n",
        "\tplt.plot(range(iterations), costs, color='b')\n",
        "\th.show()\n",
        "\n",
        "\t# if user wants to predict using the regressor:\n",
        "\tregressor.predict([i for i in range(10)])\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\tmain()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}